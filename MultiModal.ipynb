{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1978289,"sourceType":"datasetVersion","datasetId":1182338},{"sourceId":11478584,"sourceType":"datasetVersion","datasetId":7194258}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"0bde548b-5758-49c1-8687-9eb8d032efd7","cell_type":"code","source":"# !pip install facenet-pytorch==2.*\n# !pip install torch torchvision torchaudio==2.* librosa==0.* opencv-python==4.* tqdm einops matplotlib\n# !pip install --upgrade --force-reinstall numpy scikit-learn --quiet\n# !pip install --force-reinstall \"numpy<2.1\" \"numba>=0.59\" --quiet","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"0b262d83","cell_type":"code","source":"\nimport os, random, csv\nfrom pathlib import Path\nfrom typing import List, Tuple, Dict, Any, Optional\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ntry:\n    import torchvision\nexcept Exception as e:\n    torchvision = None\n    print(\"torchvision not available:\", e)\n\ntry:\n    import librosa\nexcept Exception as e:\n    librosa = None\n    print(\"librosa not available:\", e)\n\ntry:\n    import torchaudio\nexcept Exception as e:\n    torchaudio = None\n    print(\"torchaudio not available:\", e)\n\ntry:\n    import cv2\nexcept Exception as e:\n    cv2 = None\n    print(\"opencv-python not available:\", e)\n\ntry:\n    from facenet_pytorch import MTCNN\nexcept Exception as e:\n    MTCNN = None\n    print(\"facenet-pytorch not available:\", e)\n\nfrom dataclasses import dataclass, field\nfrom tqdm import tqdm\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", DEVICE)\ntorch.backends.cudnn.benchmark = True\n\ndef set_seed(seed: int = 42):\n    random.seed(seed); np.random.seed(seed)\n    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n\ndef count_params(model: nn.Module) -> int:\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ndef pad_sequence_1d(seqs: List[torch.Tensor], pad_value: float = 0.0) -> torch.Tensor:\n    max_t = max(s.shape[0] for s in seqs)\n    if seqs[0].dim() == 2:\n        d = seqs[0].shape[1]\n        out = torch.full((len(seqs), max_t, d), pad_value, dtype=seqs[0].dtype)\n        for i, s in enumerate(seqs):\n            out[i, :s.shape[0]] = s\n    else:\n        out = torch.full((len(seqs), max_t), pad_value, dtype=seqs[0].dtype)\n        for i, s in enumerate(seqs):\n            out[i, :s.shape[0]] = s\n    return out\n\ndef lengths_to_mask(lengths: torch.Tensor, max_len: Optional[int] = None) -> torch.Tensor:\n    if max_len is None: max_len = int(lengths.max().item())\n    range_row = torch.arange(max_len, device=lengths.device).unsqueeze(0)\n    return range_row < lengths.unsqueeze(1)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"8604e6f9","cell_type":"code","source":"\n@dataclass\nclass Config:\n    ravdess_root: str = \"/kaggle/input/ravdess-dataset\"  # parent containing Audio_* and Video_*\n    meld_root: str = \"/kaggle/input/meld-dataset/MELD-RAW/MELD.Raw\"          # Kaggle zaber666 root (csv + video/train|dev|test [+ optional audio/*])\n    workdir: str = \"./artifacts_ravdess_meld_splitdirs\"\n\n    # Audio\n    sample_rate: int = 16000\n    audio_duration_s: float = 2.45\n    audio_trim_leading_s: float = 0.5\n    n_mfcc: int = 13\n    n_fft: int = 512\n    hop_length: int = 160\n    win_length: int = 400\n\n    # Video\n    frames_per_clip: int = 16\n    image_size: int = 112\n    face_crop: bool = True\n\n    # 7-class\n    classes7: List[str] = field(default_factory=lambda: [\"angry\",\"disgust\",\"fear\",\"happy\",\"neutral\",\"sad\",\"surprise\"])\n\n    # Training\n    seed: int = 42\n    batch_size: int = 8\n    num_workers: int = 4\n    epochs: int = 25\n    lr: float = 1e-3\n    weight_decay: float = 1e-4\n    grad_clip: float = 5.0\n\n    # Model\n    audio_cnn_channels: Tuple[int, ...] = (32, 64, 128)\n    audio_lstm_hidden: int = 128\n    video_backbone: str = \"resnet18\"\n    video_lstm_hidden: int = 256\n    transformer_d_model: int = 128\n    transformer_nhead: int = 4\n    transformer_num_layers: int = 2\n    fusion_k: int = 64\n    dropout: float = 0.3\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"7d007636","cell_type":"code","source":"def load_audio(path: str, cfg: Config) -> np.ndarray:\n    \"\"\"\n    Load audio from an audio file (.wav/.flac/.mp3, etc.) first.\n    If that fails, fall back to extracting audio from a video file (.mp4/.mkv/.avi/.mov).\n    \n    Returns a trimmed + padded numpy array of length cfg.audio_duration_s * cfg.sample_rate.\n    \"\"\"\n    import numpy as np\n    import os\n\n    y, sr = None, None\n    dur = int(cfg.audio_duration_s * cfg.sample_rate)\n    start = int(cfg.audio_trim_leading_s * cfg.sample_rate)\n\n    try:\n        # --- Try as standard audio first ---\n        import librosa\n        y, sr = librosa.load(path, sr=cfg.sample_rate)\n    except Exception as e_audio:\n        print(f\"[WARN] Audio read failed for {os.path.basename(path)} ({e_audio}). Trying video backend...\")\n        try:\n            # --- Fallback: extract audio from video via FFmpeg ---\n            import torchaudio\n            wav, sr = torchaudio.load(path, format=\"mp4\")\n            if sr != cfg.sample_rate:\n                wav = torchaudio.functional.resample(wav, sr, cfg.sample_rate)\n                sr = cfg.sample_rate\n            y = wav.mean(dim=0).numpy()\n        except Exception as e_video:\n            print(f\"[ERROR] Failed to extract audio from {os.path.basename(path)} ({e_video}). Returning silence.\")\n            y = np.zeros(dur)\n            sr = cfg.sample_rate\n\n    # --- Trim and pad to fixed duration ---\n    y = y[start : start + dur]\n    if len(y) < dur:\n        y = np.pad(y, (0, dur - len(y)))\n\n    return y\n\ndef audio_to_mfcc(y: np.ndarray, cfg: Config) -> np.ndarray:\n    mfcc = librosa.feature.mfcc(\n        y=y, sr=cfg.sample_rate,\n        n_mfcc=cfg.n_mfcc,\n        n_fft=cfg.n_fft,\n        hop_length=cfg.hop_length,\n        win_length=cfg.win_length\n    )\n    return mfcc.T.astype(np.float32)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e60f2a61","cell_type":"code","source":"\ndef get_mtcnn(image_size: int):\n    if MTCNN is None: return None\n    return MTCNN(image_size=image_size, margin=20, post_process=True, keep_all=False, device=str(DEVICE))\n\ndef center_crop_resize(img: np.ndarray, size: int) -> np.ndarray:\n    h, w = img.shape[:2]\n    scale = size / min(h, w)\n    img = cv2.resize(img, (int(w*scale), int(h*scale)))\n    h, w = img.shape[:2]\n    y0 = (h - size)//2; x0 = (w - size)//2\n    return img[y0:y0+size, x0:x0+size]\n\ndef load_video_frames_cv2(path: str, cfg: Config, mtcnn=None) -> List[np.ndarray]:\n    assert cv2 is not None, \"opencv-python not installed\"\n    cap = cv2.VideoCapture(path)\n    frames = []\n    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n    idxs = np.linspace(0, max(total-1, 1), cfg.frames_per_clip).astype(int) if total > 0 else None\n    i_sel = 0; i_count = 0\n    while True:\n        ret, frame = cap.read()\n        if not ret: break\n        if idxs is None or i_count == idxs[i_sel]:\n            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            if cfg.face_crop and mtcnn is not None:\n                try:\n                    face = mtcnn(frame_rgb, save_path=None)\n                    if face is not None:\n                        img = face.permute(1,2,0).byte().cpu().numpy()\n                    else:\n                        img = center_crop_resize(frame_rgb, cfg.image_size)\n                except Exception:\n                    img = center_crop_resize(frame_rgb, cfg.image_size)\n            else:\n                img = center_crop_resize(frame_rgb, cfg.image_size)\n            frames.append(img)\n            if total > 0 and i_sel < len(idxs)-1: i_sel += 1\n            if len(frames) >= cfg.frames_per_clip: break\n        i_count += 1\n    cap.release()\n    if len(frames) == 0:\n        frames = [np.zeros((cfg.image_size, cfg.image_size, 3), dtype=np.uint8) for _ in range(cfg.frames_per_clip)]\n    if len(frames) < cfg.frames_per_clip:\n        frames += [frames[-1]] * (cfg.frames_per_clip - len(frames))\n    return frames\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"954facd0","cell_type":"code","source":"\nEMO_MAP_RAVDESS = {\n    \"01\": \"neutral\",\n    \"02\": \"calm\",\n    \"03\": \"happy\",\n    \"04\": \"sad\",\n    \"05\": \"angry\",\n    \"06\": \"fearful\",\n    \"07\": \"disgust\",\n    \"08\": \"surprised\",\n}\nRAVDESS_TO_7 = {\n    \"neutral\": \"neutral\",\n    \"calm\": \"neutral\",\n    \"happy\": \"happy\",\n    \"sad\": \"sad\",\n    \"angry\": \"angry\",\n    \"fearful\": \"fear\",\n    \"disgust\": \"disgust\",\n    \"surprised\": \"surprise\",\n}\n\ndef index_ravdess_7(root: str) -> List[Dict[str, Any]]:\n    root = Path(root)\n    video_dirs = [p for p in root.iterdir() if p.is_dir() and \"video\" in p.name.lower()]\n    search_bases = video_dirs if video_dirs else [root]\n    mp4s = []\n    for b in search_bases:\n        mp4s.extend(b.rglob(\"*.mp4\"))\n    if not mp4s: mp4s = list(root.rglob(\"*.mp4\"))\n    items = []\n    for mp4 in mp4s:\n        parts = mp4.stem.split(\"-\")\n        if len(parts) < 3: continue\n        emo_code = parts[2]\n        emo_full = EMO_MAP_RAVDESS.get(emo_code)\n        if emo_full is None: continue\n        mapped = RAVDESS_TO_7.get(emo_full, None)\n        if mapped is None: continue\n        # Find WAV (mirror folder) or fallback to mp4\n        wav_path = None\n        for parent in mp4.parents:\n            if \"video\" in parent.name.lower():\n                alt = parent.with_name(parent.name.replace(\"Video\", \"Audio\").replace(\"video\", \"Audio\"))\n                cand = alt / mp4.name.replace(\".mp4\", \".wav\")\n                if cand.exists():\n                    wav_path = cand; break\n        if wav_path is None:\n            cand = root / mp4.name.replace(\".mp4\", \".wav\")\n            if cand.exists(): wav_path = cand\n        audio_path = wav_path if wav_path is not None else mp4\n        actor_id = \"00\"\n        for par in mp4.parents:\n            if par.name.startswith(\"Actor_\"):\n                actor_id = par.name.split(\"_\")[-1]; break\n        items.append({\"video\": str(mp4), \"audio\": str(audio_path), \"label7\": mapped, \"actor\": actor_id})\n    return items\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"3655c857","cell_type":"code","source":"\nMELD_TO_7 = {\n    \"anger\": \"angry\",\n    \"disgust\": \"disgust\",\n    \"fear\": \"fear\",\n    \"joy\": \"happy\",\n    \"neutral\": \"neutral\",\n    \"sadness\": \"sad\",\n    \"surprise\": \"surprise\",\n}\n\ndef _csv_rows(csv_path: Path) -> List[Dict[str, Any]]:\n    rows = []\n    if not csv_path.exists(): return rows\n    with csv_path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n        reader = csv.DictReader(f)\n        for r in reader: rows.append(r)\n    return rows\n\ndef index_meld_7_splitdirs(root: str, include_splits=(\"train\",\"dev\",\"test\")) -> List[Dict[str, Any]]:\n    root = Path(root)\n    split_to_csv = {\n        \"train\": root / \"train\" / \"train_sent_emo.csv\",\n        \"dev\": root / \"dev_sent_emo.csv\",\n        \"test\": root / \"test_sent_emo.csv\",\n    }\n    items = []\n    for split in include_splits:\n        rows = _csv_rows(split_to_csv[split])\n        if split == \"train\":\n            video_dir = root / split / \"train_splits\"\n        elif split == \"dev\":\n            video_dir = root / split / \"dev_splits_complete\"\n        elif split == \"test\":\n            video_dir = root / split / \"output_repeated_splits_test\"\n        \n        audio_dir = root / \"audio\" / split  # optional\n        for r in rows:\n            # ints in the CSV\n            try:\n                dia = int(r.get(\"Dialogue_ID\", r.get(\"dialogue_id\", r.get(\"Dialogue_ID\".lower()))))\n                utt = int(r.get(\"Utterance_ID\", r.get(\"utterance_id\", r.get(\"Utterance_ID\".lower()))))\n            except Exception:\n                continue\n            emo_raw = (r.get(\"Emotion\") or r.get(\"emotion\") or \"\").strip().lower()\n            mapped = MELD_TO_7.get(emo_raw, None)\n            if mapped is None: continue\n            key = f\"dia{dia}_utt{utt}\"\n            vid = None; wav = None\n            # prefer per-split video\n            cand_vid = video_dir / f\"{key}.mp4\"\n            if not cand_vid.exists():\n                # try other common video extensions\n                for ext in (\".avi\",\".mov\",\".mkv\"):\n                    cv = video_dir / f\"{key}{ext}\"\n                    if cv.exists(): cand_vid = cv; break\n            if cand_vid.exists(): vid = cand_vid\n            # audio optional\n            cand_wav = audio_dir / f\"{key}.wav\"\n            if cand_wav.exists(): wav = cand_wav\n            # only accept if we have video; if no wav, we'll read audio from video later\n            if vid is None: continue\n            items.append({\n                \"video\": str(vid),\n                \"audio\": str(wav if wav is not None else vid),\n                \"label7\": mapped,\n                \"split\": split,\n                \"utt\": key,\n            })\n    return items\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"411d33e9","cell_type":"code","source":"\nclass AVItem:\n    __slots__ = (\"video\", \"audio\", \"label7\", \"meta\")\n    def __init__(self, video: str, audio: str, label7: str, meta: Dict[str, Any]):\n        self.video, self.audio, self.label7, self.meta = video, audio, label7, meta\n\nclass AVDataset(Dataset):\n    def __init__(self, items: List[Dict[str, Any]], label_list: List[str], cfg: Config):\n        self.items = []\n        self.label_to_idx = {c:i for i,c in enumerate(label_list)}\n        self.cfg = cfg\n        self.mtcnn = get_mtcnn(cfg.image_size) if cfg.face_crop else None\n        for it in items:\n            if not Path(it[\"video\"]).exists(): continue\n            if not Path(it[\"audio\"]).exists():\n                # audio path might be a video file (fallback extraction)\n                if not Path(it[\"audio\"]).exists(): pass\n            meta = {k:v for k,v in it.items() if k not in (\"video\",\"audio\",\"label7\")}\n            self.items.append(AVItem(it[\"video\"], it[\"audio\"], it[\"label7\"], meta))\n        print(f\"AVDataset: {len(self.items)} items\")\n\n    def __len__(self) -> int: return len(self.items)\n\n    def __getitem__(self, idx) -> Dict[str, Any]:\n        it = self.items[idx]\n        y = load_audio(it.audio, self.cfg)\n        mfcc = audio_to_mfcc(y, self.cfg)\n        a_len = mfcc.shape[0]\n        frames = load_video_frames_cv2(it.video, self.cfg, self.mtcnn)\n        vid = np.stack(frames, 0).astype(np.float32) / 255.0\n        vid = np.transpose(vid, (0,3,1,2))\n        label_idx = self.label_to_idx[it.label7]\n        return {\"mfcc\": torch.from_numpy(mfcc),\n                \"a_len\": torch.tensor(a_len, dtype=torch.long),\n                \"video\": torch.from_numpy(vid),\n                \"label\": torch.tensor(label_idx, dtype=torch.long),\n                \"meta\": it.meta}\n\ndef av_collate(batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n    mfccs = [b[\"mfcc\"] for b in batch]\n    a_lens = torch.tensor([b[\"a_len\"] for b in batch], dtype=torch.long)\n    labels = torch.stack([b[\"label\"] for b in batch])\n    mfcc_p = pad_sequence_1d(mfccs)\n    vids = torch.stack([b[\"video\"] for b in batch], dim=0)\n    return {\"mfcc\": mfcc_p.float(), \"a_len\": a_lens, \"video\": vids.float(), \"label\": labels}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a683bfa2","cell_type":"code","source":"\nclass Conv1DBlock(nn.Module):\n    def __init__(self, in_ch, out_ch, k=7, p=3, s=1, dropout=0.0):\n        super().__init__()\n        self.conv = nn.Conv1d(in_ch, out_ch, kernel_size=k, padding=p, stride=s)\n        self.bn = nn.BatchNorm1d(out_ch)\n        self.drop = nn.Dropout(dropout)\n    def forward(self, x):\n        x = self.conv(x); x = F.relu(self.bn(x)); return self.drop(x)\n\nclass AudioEncoder(nn.Module):\n    def __init__(self, cfg: Config):\n        super().__init__()\n        c = cfg.audio_cnn_channels\n        self.conv1 = Conv1DBlock(cfg.n_mfcc, c[0], k=7, p=3, dropout=cfg.dropout)\n        self.conv2 = Conv1DBlock(c[0], c[1], k=7, p=3, dropout=cfg.dropout)\n        self.conv3 = Conv1DBlock(c[1], c[2], k=5, p=2, dropout=cfg.dropout)\n        self.pool = nn.MaxPool1d(kernel_size=2)\n        self.lstm = nn.LSTM(input_size=c[-1], hidden_size=cfg.audio_lstm_hidden, num_layers=1, batch_first=True, bidirectional=True)\n        self.proj = nn.Linear(2*cfg.audio_lstm_hidden, cfg.transformer_d_model)\n        enc_layer = nn.TransformerEncoderLayer(d_model=cfg.transformer_d_model, nhead=cfg.transformer_nhead,\n                                               dim_feedforward=4*cfg.transformer_d_model, batch_first=True,\n                                               dropout=cfg.dropout, activation=\"relu\")\n        self.self_attn = nn.TransformerEncoder(enc_layer, num_layers=cfg.transformer_num_layers)\n        self.out_dim = cfg.transformer_d_model\n    def forward(self, mfcc: torch.Tensor, lengths: torch.Tensor):\n        x = mfcc.transpose(1,2)\n        x = self.conv1(x); x = self.pool(x)\n        x = self.conv2(x); x = self.pool(x)\n        x = self.conv3(x); x = x.transpose(1,2)\n        x_packed = nn.utils.rnn.pack_padded_sequence(x, lengths.cpu()//4, batch_first=True, enforce_sorted=False)\n        x_enc, _ = self.lstm(x_packed)\n        x_enc, _ = nn.utils.rnn.pad_packed_sequence(x_enc, batch_first=True)\n        x_proj = self.proj(x_enc)\n        x_sel = self.self_attn(x_proj)\n        mask = lengths_to_mask(lengths//4, x_sel.size(1)).unsqueeze(-1)\n        x_sel_masked = x_sel * mask\n        denom = mask.sum(dim=1).clamp(min=1)\n        a_summary = x_sel_masked.sum(dim=1) / denom\n        return x_sel, a_summary\n\nclass FrameCNN(nn.Module):\n    def __init__(self, backbone=\"resnet18\"):\n        super().__init__()\n        assert torchvision is not None, \"torchvision not installed\"\n        if backbone == \"resnet18\":\n            net = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.DEFAULT)\n            modules = list(net.children())[:-1]\n            self.backbone = nn.Sequential(*modules)\n            self.out_dim = 512\n        else: raise ValueError(\"Unsupported backbone\")\n    def forward(self, x):\n        return self.backbone(x).flatten(1)\n\nclass VideoEncoder(nn.Module):\n    def __init__(self, cfg: Config):\n        super().__init__()\n        self.frame_cnn = FrameCNN(backbone=cfg.video_backbone)\n        self.lstm = nn.LSTM(input_size=self.frame_cnn.out_dim, hidden_size=cfg.video_lstm_hidden, num_layers=1, batch_first=True, bidirectional=True)\n        self.out_dim = 2*cfg.video_lstm_hidden\n    def forward(self, video: torch.Tensor):\n        B, T, C, H, W = video.shape\n        x = video.reshape(B*T, C, H, W)\n        feats = self.frame_cnn(x).view(B, T, -1)\n        v_seq, _ = self.lstm(feats)\n        v_summary = v_seq.mean(dim=1)\n        return v_seq, v_summary\n\nclass CrossModalFusion(nn.Module):\n    def __init__(self, v_dim: int, a_dim: int, k: int):\n        super().__init__()\n        self.Wv = nn.Linear(v_dim, k, bias=True)\n        self.Wa = nn.Linear(a_dim, k, bias=False)\n        self.softmax = nn.Softmax(dim=1)\n    def forward(self, v_seq: torch.Tensor, a_summary: torch.Tensor) -> torch.Tensor:\n        Pv = self.Wv(v_seq)\n        Pa = self.Wa(a_summary).unsqueeze(1)\n        Xq = torch.tanh(Pv + Pa)\n        alpha = self.softmax(Xq.mean(dim=-1)).unsqueeze(-1)\n        return v_seq + alpha * v_seq\n\nclass AVFusionModel(nn.Module):\n    def __init__(self, cfg: Config, n_classes: int):\n        super().__init__()\n        self.audio_enc = AudioEncoder(cfg)\n        self.video_enc = VideoEncoder(cfg)\n        self.fusion = CrossModalFusion(self.video_enc.out_dim, self.audio_enc.out_dim, cfg.fusion_k)\n        fused_dim = self.video_enc.out_dim + self.audio_enc.out_dim\n        self.cls = nn.Sequential(nn.LayerNorm(fused_dim), nn.Dropout(cfg.dropout), nn.Linear(fused_dim, n_classes))\n    def forward(self, mfcc, a_len, video):\n        _, a_summary = self.audio_enc(mfcc, a_len)\n        v_seq, _ = self.video_enc(video)\n        v_fused = self.fusion(v_seq, a_summary)\n        v_pooled = v_fused.mean(dim=1)\n        fused = torch.cat([v_pooled, a_summary], dim=-1)\n        return self.cls(fused)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"bd7c16a5","cell_type":"code","source":"\ndef accuracy_score_simple(y_true, y_pred):\n    yt = torch.as_tensor(y_true); yp = torch.as_tensor(y_pred)\n    return (yt == yp).float().mean().item()\n\ndef f1_weighted_simple(y_true, y_pred, n_classes):\n    yt = torch.as_tensor(y_true, dtype=torch.long)\n    yp = torch.as_tensor(y_pred, dtype=torch.long)\n    f1s, supports = [], []\n    for c in range(n_classes):\n        tp = ((yp == c) & (yt == c)).sum().item()\n        fp = ((yp == c) & (yt != c)).sum().item()\n        fn = ((yp != c) & (yt == c)).sum().item()\n        support = (yt == c).sum().item()\n        denom = (2*tp + fp + fn)\n        f1 = (2*tp / denom) if denom > 0 else 0.0\n        f1s.append(f1); supports.append(support)\n    total = sum(supports) if sum(supports) > 0 else 1\n    return sum(f*s for f, s in zip(f1s, supports)) / total\n\ndef train_one_epoch(model, loader, optim, scheduler, cfg: Config):\n    model.train()\n    total_loss, correct, total = 0.0, 0, 0\n    for batch in tqdm(loader, desc=\"Train\", leave=False):\n        mfcc = batch[\"mfcc\"].to(DEVICE)\n        a_len = batch[\"a_len\"].to(DEVICE)\n        video = batch[\"video\"].to(DEVICE)\n        labels = batch[\"label\"].to(DEVICE)\n\n        optim.zero_grad()\n        logits = model(mfcc, a_len, video)\n        loss = F.cross_entropy(logits, labels)\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n        optim.step()\n\n        total_loss += loss.item() * labels.size(0)\n        preds = logits.argmax(dim=-1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n    if scheduler is not None:\n        scheduler.step(total_loss / max(1,total))\n    return total_loss / max(1,total), correct / max(1,total)\n\n@torch.no_grad()\ndef evaluate(model, loader):\n    model.eval()\n    total_loss, y_true, y_pred = 0.0, [], []\n    for batch in tqdm(loader, desc=\"Eval\", leave=False):\n        mfcc = batch[\"mfcc\"].to(DEVICE)\n        a_len = batch[\"a_len\"].to(DEVICE)\n        video = batch[\"video\"].to(DEVICE)\n        labels = batch[\"label\"].to(DEVICE)\n        logits = model(mfcc, a_len, video)\n        loss = F.cross_entropy(logits, labels)\n        total_loss += loss.item() * labels.size(0)\n        y_true.extend(labels.cpu().tolist())\n        y_pred.extend(logits.argmax(dim=-1).cpu().tolist())\n    n_classes = int(max(max(y_true), max(y_pred)) + 1) if y_true else 1\n    acc = accuracy_score_simple(y_true, y_pred)\n    f1w = f1_weighted_simple(y_true, y_pred, n_classes=n_classes)\n    return total_loss / max(1,len(y_true)), acc, f1w, (y_true, y_pred)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"6b93875e","cell_type":"code","source":"\ndef kfold_from_actors(items: List[Dict[str, Any]], k: int = 5) -> List[Tuple[List[int], List[int]]]:\n    from collections import defaultdict\n    by_actor = defaultdict(list)\n    for i, it in enumerate(items):\n        a = it.get(\"actor\", \"00\"); by_actor[a].append(i)\n    actors = sorted(by_actor.keys())\n    folds = [[] for _ in range(k)]\n    for i, a in enumerate(actors):\n        folds[i % k].extend(by_actor[a])\n    splits = []\n    for i in range(k):\n        test_idx = folds[i]\n        train_idx = [j for t in range(k) if t != i for j in folds[t]]\n        splits.append((train_idx, test_idx))\n    return splits\n\ndef meld_standard_split(items: List[Dict[str, Any]]):\n    idx_train = [i for i,x in enumerate(items) if x.get(\"split\")==\"train\"]\n    idx_dev   = [i for i,x in enumerate(items) if x.get(\"split\")==\"dev\"]\n    idx_test  = [i for i,x in enumerate(items) if x.get(\"split\")==\"test\"]\n    return idx_train, idx_dev, idx_test\n\ndef make_loader(items, labels, cfg: Config, shuffle: bool):\n    ds = AVDataset(items, labels, cfg)\n    dl = DataLoader(ds, batch_size=cfg.batch_size, shuffle=shuffle, num_workers=cfg.num_workers,\n                    collate_fn=av_collate, pin_memory=True)\n    return ds, dl\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"02cbcadd","cell_type":"code","source":"import os\nimport torch\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.model_selection import train_test_split\n\ndef train_on_ravdess_eval_meld(cfg: Config):\n    labels = cfg.classes7\n\n    # Load datasets\n    rav = index_ravdess_7(cfg.ravdess_root)\n    assert len(rav) > 0, \"No RAVDESS items found (7-class).\"\n    meld = index_meld_7_splitdirs(cfg.meld_root, include_splits=(\"train\",\"dev\",\"test\"))\n    assert len(meld) > 0, \"No MELD items found (7-class).\"\n\n    # Split RAVDESS into train/test\n    tr_items, te_items = train_test_split(rav, test_size=0.2, random_state=cfg.seed, shuffle=True)\n    _, _, meld_test_idx = meld_standard_split(meld)\n    meld_test = [meld[i] for i in meld_test_idx]\n\n    print(f\"RAVDESS train: {len(tr_items)}, test: {len(te_items)}, MELD test: {len(meld_test)}\")\n\n    # Data loaders\n    set_seed(cfg.seed)\n    _, train_dl = make_loader(tr_items, labels, cfg, shuffle=True)\n    _, test_dl_r = make_loader(te_items, labels, cfg, shuffle=False)\n    _, test_dl_m = make_loader(meld_test, labels, cfg, shuffle=False)\n\n    # Model setup\n    model = AVFusionModel(cfg, n_classes=len(labels)).to(DEVICE)\n    print(\"Params:\", count_params(model))\n    optim = Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n    sched = ReduceLROnPlateau(optim, mode=\"min\", patience=3, factor=0.5, verbose=True)\n\n    best = {\"acc_r\": 0.0, \"state\": None}\n\n    # Training loop\n    for epoch in range(1, cfg.epochs + 1):\n        tl, ta = train_one_epoch(model, train_dl, optim, sched, cfg)\n        vl, va, vf1, _ = evaluate(model, test_dl_r)\n        print(f\"[RAV train/test] Epoch {epoch:02d} | train {tl:.4f}/{ta:.3f} | val {vl:.4f}/{va:.3f} f1w {vf1:.3f}\")\n        if va > best[\"acc_r\"]:\n            best[\"acc_r\"] = va\n            best[\"state\"] = {k: v.cpu() for k, v in model.state_dict().items()}\n\n    # Evaluate best model\n    model.load_state_dict(best[\"state\"])\n    _, acc_r, f1_r, _ = evaluate(model, test_dl_r)\n    _, acc_m, f1_m, _ = evaluate(model, test_dl_m)\n    print(f\"Best -> RAVDESS acc {acc_r:.3f}, MELD acc {acc_m:.3f}\")\n\n    return (acc_r, f1_r, acc_m, f1_m)\n\n\ndef train_on_meld_eval_ravdess(cfg: Config):\n    labels = cfg.classes7\n    meld = index_meld_7_splitdirs(cfg.meld_root, include_splits=(\"train\",\"dev\",\"test\"))\n    assert len(meld) > 0, \"No MELD items found (7-class).\"\n    idx_tr, idx_dev, idx_te = meld_standard_split(meld)\n    meld_train = [meld[i] for i in (idx_tr + idx_dev)]\n    meld_test  = [meld[i] for i in idx_te]\n\n    rav = index_ravdess_7(cfg.ravdess_root)\n    assert len(rav) > 0, \"No RAVDESS items found (7-class).\"\n\n    set_seed(cfg.seed)\n    _, train_dl = make_loader(meld_train, labels, cfg, shuffle=True)\n    _, test_dl_m = make_loader(meld_test, labels, cfg, shuffle=False)\n    _, test_dl_r = make_loader(rav, labels, cfg, shuffle=False)\n\n    model = AVFusionModel(cfg, n_classes=len(labels)).to(DEVICE)\n    print(\"Params:\", count_params(model))\n    optim = Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n    sched = ReduceLROnPlateau(optim, mode=\"min\", patience=3, factor=0.5, verbose=True)\n\n    best = {\"acc_m\":0.0, \"state\":None}\n    for epoch in range(1, cfg.epochs+1):\n        tl, ta = train_one_epoch(model, train_dl, optim, sched, cfg)\n        vl, va, vf1, _ = evaluate(model, test_dl_m)\n        print(f\"[MELD] Epoch {epoch:02d} | train {tl:.4f}/{ta:.3f} | val {vl:.4f}/{va:.3f} f1w {vf1:.3f}\")\n        if va > best[\"acc_m\"]:\n            best[\"acc_m\"] = va\n            best[\"state\"] = {k:v.cpu() for k,v in model.state_dict().items()}\n    model.load_state_dict(best[\"state\"])\n    _, acc_m, f1_m, _ = evaluate(model, test_dl_m)\n    _, acc_r, f1_r, _ = evaluate(model, test_dl_r)\n    print(f\"MELD-trained best -> MELD acc {acc_m:.3f}, RAVDESS acc {acc_r:.3f}\")\n    return (acc_m, f1_m, acc_r, f1_r)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"bd4a00a5","cell_type":"code","source":"# Dry-run shapes\nset_seed(7)\ncfg = Config()\nB, Ta, Tv = 2, 200, cfg.frames_per_clip\nmfcc = torch.randn(B, Ta, cfg.n_mfcc).to(DEVICE)\na_len = torch.tensor([Ta, Ta-10], dtype=torch.long).to(DEVICE)\nvideo = torch.randn(B, Tv, 3, cfg.image_size, cfg.image_size).to(DEVICE)\nmodel = AVFusionModel(cfg, n_classes=len(cfg.classes7)).to(DEVICE)\nwith torch.no_grad():\n    logits = model(mfcc, a_len, video)\nprint(\"Logits shape:\", logits.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"6bc29465-56ea-4a8f-b8cb-2f02896e9fc9","cell_type":"code","source":"cfg = Config(\n    ravdess_root=\"/kaggle/input/ravdess-dataset\",\n    meld_root=\"/kaggle/input/meld-dataset/MELD-RAW/MELD.Raw\",   # folder that contains csvs and video/train|dev|test (and optional audio/..)\n    workdir=\"./artifacts_ravdess_meld_splitdirs\",\n    face_crop=True,\n    epochs=10,\n    batch_size=32,\n)\n\nrav_to_meld = train_on_ravdess_eval_meld(cfg)\nmeld_to_rav = train_on_meld_eval_ravdess(cfg)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}